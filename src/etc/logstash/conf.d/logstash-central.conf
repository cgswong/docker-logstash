# Where to get input
input {
  # Get input from Redis queue
  redis {
    host => "127.0.0.1"
    type => "redis-input"
    data_type => "list"
    key => "logstash"
  }
  # Get input from syslog over port 5000
  tcp {
    port => 5000
    type => "syslog"
  }
  udp {
    port => 5000
    type => "syslog"
  }
  # Get Lumberjack over TCP port 5002
  lumberjack {
    port            => 5002
    ssl_certificate => "/etc/logstash/ssl/logstash-forwarder.crt"
    ssl_key         => "/etc/logstash/ssl/logstash-forwarder.key"
    type            => "syslog"
  }
  # Get input from CoreOS journal
  tcp {
    port  => 5004
    codec => json_lines
    type  => "systemd"
  }
  # Get input over TCP port 5100 as JSON lines
  tcp {
    type  => "json"
    port  => 5100
    codec => json_lines
  }
  # Get application logs via log4j over TCP port 5200
  log4j {
    port => 5200
  }
}

# Some Filtering
filter {
  # SYSLOG filter
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    syslog_pri { }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }

    if !("_grokparsefailure" in [tags]) {
      mutate {
        replace => [ "message", "%{syslog_message}" ]
      }

      mutate {
        remove_field => [  "syslog_message" ]
      }
    }

    # Remove spurious fields that have names changed or been aggregated
    mutate {
      remove_field => [ "syslog_hostname", "syslog_timestamp" ]
    }
  }

  # Docker filter
  if [type] == "docker" {
    json {
      source => "message"
    }
    mutate {
      rename => [ "log", "message" ]
    }
    date {
      match => [ "time", "ISO8601" ]
    }
  }
}

# Where to send output
output {
  # Send output to standard output device/interface
  stdout {
    codec => rubydebug
  }

  # Parse failed syslog messages
  if [type] == "syslog" and "_grokparsefailure" in [tags] {
    file { path => "/var/log/failed_syslog_events-%{+YYYY-MM-dd}" }
  }

# Send output to Elasticsearch over HTTP interface.
  elasticsearch {
    protocol => 'http'
    cluster  => "ES_CLUSTER_NAME"
    host     => "ES_PORT_9200_TCP_ADDR"
    port     => "ES_PORT_9200_TCP_PORT"
  }
# Send output metrics to statsd for statistics aggregation
#  statsd {
    # Count one hit every event by response
#    increment => "apache.response.%{response}"
    # Use the 'bytes' field from the apache log as the count value.
#    count => [ "apache.bytes", "%{bytes}" ]
#  }
#  statsd {
#    host  => 'graphite.example.org'
#    count => [ "tomcat.bytes", "%{bytes}" ]
#  }
#  statsd {
#    host      => 'graphite.example.org'
#    increment => "tomcat.response.%{response}"
#  }
#  statsd {
#    host    => 'graphite.example.org'
#    timing  => [ "tomcat.indextime", "%{indextime}" ]
#  }
}
