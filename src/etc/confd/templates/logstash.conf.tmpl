# #####################################################################
# NAME: logstash.conf
# DESC: Logstash configuration file. Typically forwarding logs to
#       Elasticsearch instance.
#
# LOG:
# yyyy/mm/dd [user] [version]: [notes]
# 2014/10/23 cgwong [v0.1.0]: Initial creation.
# 2014/11/11 cgwong v0.2.0: Updated environment variables.
# 2015/01/14 cgwong v0.3.0: Added additional port for remote syslog and filtering.
# 2015/01/28 cgwong v0.4.0: Clean up Elasticsearch output stream. Add additional plugins.
# 2015/02/20 cgwong v0.5.0: Change ports.
# #####################################################################

# Where to get input
input {
  # Get input from syslog over port 5000
  tcp {
    port => 5000
    type => "syslog"
  }
  udp {
    port => 5000
    type => "syslog"
  }
  # Get Lumberjack over TCP port 5002
#  lumberjack {
#    port            => 5002
#    ssl_certificate => "/etc/logstash/ssl/logstash-forwarder.crt"
#    ssl_key         => "/etc/logstash/ssl/logstash-forwarder.key"
#    type            => "syslog"
#  }
  # Get input from CoreOS journal
  tcp {
    port  => 5004
    codec => json_lines
    type  => "systemd"
  }
  # Get input over TCP port 5100 as JSON lines
  tcp {
    type  => "json"
    port  => 5100
    codec => json_lines
  }
  # Get application logs via log4j over TCP port 5200
  log4j {
    port => 5200
  }
}

# Some Filtering
filter {
  # SYSLOG filter
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    syslog_pri { }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }

    if !("_grokparsefailure" in [tags]) {
      mutate {
        replace => [ "message", "%{syslog_message}" ]
      }

      mutate {
        remove_field => [  "syslog_message" ]
      }
    }

    # Remove spurious fields that have names changed or been aggregated
    mutate {
      remove_field => [ "syslog_hostname", "syslog_timestamp" ]
    }
  }

  if [type] == "systemd" {
    mutate { rename => [ "MESSAGE", "message" ] }
    mutate { rename => [ "_SYSTEMD_UNIT", "program" ] }
  }

  # Docker filter
  if [type] == "docker" {
    json {
      source => "message"
    }
    mutate {
      rename => [ "log", "message" ]
    }
    date {
      match => [ "time", "ISO8601" ]
    }
  }
}

# Where to send output
output {
  # Send output to standard output device/interface
  stdout {
    codec => rubydebug
  }

  # Parse failed syslog messages
  if [type] == "syslog" and "_grokparsefailure" in [tags] {
    file { path => "/var/log/failed_syslog_events-%{+YYYY-MM-dd}" }
  }

# Send output to Elasticsearch over HTTP interface.
  elasticsearch {
    protocol  => "http"
    cluster   => {{getv "/services/logging/es/cluster"}}
    host      => ["{{range ls "/services/logging/es/host"}}{{.}}{{end}}:9200"]
    port      => "9200"
  }

# Send output metrics to statsd for statistics aggregation
#  statsd {
    # Count one hit every event by response
#    increment => "apache.response.%{response}"
    # Use the 'bytes' field from the apache log as the count value.
#    count => [ "apache.bytes", "%{bytes}" ]
#  }
#  statsd {
#    host  => 'graphite.example.org'
#    count => [ "tomcat.bytes", "%{bytes}" ]
#  }
#  statsd {
#    host      => 'graphite.example.org'
#    increment => "tomcat.response.%{response}"
#  }
#  statsd {
#    host    => 'graphite.example.org'
#    timing  => [ "tomcat.indextime", "%{indextime}" ]
#  }
}
